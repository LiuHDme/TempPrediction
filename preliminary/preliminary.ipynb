{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "sns.set_style('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "train.columns = ['timestamp', 'year', 'month', 'day', 'hour', 'minute', 'second', 'temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in', 'temp_in']\n",
    "test.columns = ['timestamp', 'year', 'month', 'day', 'hour', 'minute', 'second', 'temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']\n",
    "train['time'] = pd.to_datetime(train[['year', 'month', 'day', 'hour', 'minute']])\n",
    "test['time'] = pd.to_datetime(test[['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前10天：3.14 01:00 ~ 3.24 00:59  \n",
    "中间10天：3.24 1:00 ~ 4.3 00:59  \n",
    "后10天：4.3 00:59 ~ 4.13 00:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train.temp_in.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = {'year': [], 'month': [], 'day': [], 'hour': [], 'minute': []}\n",
    "for day in range(14, 32):\n",
    "    for hour in range(0, 24):\n",
    "        for minute in range(0, 60):\n",
    "            date['year'].append(2019)\n",
    "            date['month'].append(3)\n",
    "            date['day'].append(day)\n",
    "            date['hour'].append(hour)\n",
    "            date['minute'].append(minute)\n",
    "            \n",
    "for day in range(1, 14):\n",
    "    for hour in range(0, 24):\n",
    "        for minute in range(0, 60):\n",
    "            date['year'].append(2019)\n",
    "            date['month'].append(4)\n",
    "            date['day'].append(day)\n",
    "            date['hour'].append(hour)\n",
    "            date['minute'].append(minute)\n",
    "\n",
    "full_date = pd.DataFrame(date)\n",
    "full_date['time'] = pd.to_datetime(full_date[['year', 'month', 'day', 'hour', 'minute']])\n",
    "full_train = full_date.loc[(full_date.time >= '2019-3-14 01') & (full_date.time < '2019-4-3 01'), :]\n",
    "\n",
    "train = pd.merge(full_train, train, on=['year', 'month', 'day', 'hour', 'minute', 'time'], how='left')\n",
    "train.drop_duplicates(['month', 'day', 'hour', 'minute'], inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "# 顺序：前（后）一分钟，前（后）一小时，前（后）一天，上个非空值\n",
    "def fill_na(row, col):\n",
    "    if pd.isna(row[col]):\n",
    "        # 先看看前一分钟有没有\n",
    "        pre_min = row.time - pd.Timedelta('1 minute')\n",
    "        next_min = row.time + pd.Timedelta('1 minute')\n",
    "        pre_hour = row.time - pd.Timedelta('1 hour')\n",
    "        next_hour = row.time + pd.Timedelta('1 hour')\n",
    "        pre_day = row.time - pd.Timedelta('1 day')\n",
    "        next_day = row.time + pd.Timedelta('1 day')\n",
    "        times = [pre_min, next_min, pre_hour, next_hour, pre_day, next_day]\n",
    "        for time in times:\n",
    "            tmp = train.loc[train.time == time]\n",
    "            if len(tmp) != 0 and pd.notna(tmp[col].values[0]):\n",
    "                return tmp[col].values[0]\n",
    "        tmp = train.loc[(train.time < row.time) & (pd.notna(train[col]))]\n",
    "        return pre[col].values[0]\n",
    "    return row[col]\n",
    "\n",
    "train.drop_duplicates(['month', 'day', 'hour', 'minute'], inplace=True)\n",
    "\n",
    "for feat in tqdm(['temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in', 'temp_in']):\n",
    "    train[feat] = train.apply(fill_na, axis=1, args=(feat,))\n",
    "\n",
    "train['target'] = train['temp_in'] - train['temp_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 34.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "def avg_pre_next(row, col):\n",
    "    if pd.isna(row[col]):\n",
    "        pre_val = test.loc[test.time < row.time].iloc[-1][col]\n",
    "        next_val = test.loc[test.time > row.time].iloc[0][col]\n",
    "        return (pre_val + next_val) / 2\n",
    "    return row[col]\n",
    "\n",
    "for feat in tqdm(['temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']):\n",
    "    test[feat] = test.apply(avg_pre_next, axis=1, args=(feat,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(x=train.air_in)\n",
    "plt.title('train air_in')\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(x=train.air_out)\n",
    "plt.title('train air_out')\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(x=test.air_in)\n",
    "plt.title('test air_in')\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=test.air_out)\n",
    "plt.title('test air_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "train['air_out'] = exponential_smoothing(train.air_out.reset_index(drop=True), alpha)\n",
    "train['air_in'] = exponential_smoothing(train.air_in.reset_index(drop=True), alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用上一分钟和下一分钟的平均值代替异常值\n",
    "def correct_outlier(row, col, low, high, df):\n",
    "    if row[col] < low or row[col] > high:\n",
    "        time = row.time\n",
    "        pre_val = df.loc[(df.time < time) & (df[col] >= low) & (df[col] <= high)]\n",
    "        pre_val = pre_val.iloc[-1][col]\n",
    "        next_val = df.loc[(df.time > time) & (df[col] >= low) & (df[col] <= high)]\n",
    "        next_val = next_val.iloc[0][col]\n",
    "        return (pre_val + next_val) / 2\n",
    "    return row[col]\n",
    "\n",
    "train.air_in = train.apply(correct_outlier, axis=1, args=('air_in', 965, 1000, train))\n",
    "test.air_in = test.apply(correct_outlier, axis=1, args=('air_in', 500, 1000, test))\n",
    "train.air_out = train.apply(correct_outlier, axis=1, args=('air_out', 965, 1000, train))\n",
    "test.air_out = test.apply(correct_outlier, axis=1, args=('air_out', 960, 1000, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.lineplot(data=train.loc[train.time < '2019-3-24 01'].air_in)\n",
    "plt.title('3.14–3.24 air_in')\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.lineplot(data=train.loc[train.time < '2019-3-24 01'].air_out)\n",
    "plt.title('3.14–3.24 air_out')\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.lineplot(data=train.loc[train.time >= '2019-3-24 01'].air_in)\n",
    "plt.title('3.24-4.3 air_in')\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.lineplot(data=train.loc[train.time >= '2019-3-24 01'].air_out)\n",
    "plt.title('3.24–4.3 air_out')\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.lineplot(data=test.air_in)\n",
    "plt.title('4.3-4.13 air_in')\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.lineplot(data=test.air_out)\n",
    "plt.title('4.3–4.13 air_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.kdeplot(data=train.loc[train.time < '2019-3-24 01'].air_in, shade=True, label='3.14–3.24 air_in')\n",
    "sns.kdeplot(data=train.loc[train.time >= '2019-3-24 01'].air_in, shade=True, label='3.24–4.3 air_in')\n",
    "sns.kdeplot(data=test.air_in, shade=True, label='4.3-4.13 air_in')\n",
    "plt.subplot(1,2,2)\n",
    "sns.kdeplot(data=train.loc[train.time < '2019-3-24 01'].air_out, shade=True, label='3.14–3.24 air_out')\n",
    "sns.kdeplot(data=train.loc[train.time >= '2019-3-24 01'].air_out, shade=True, label='3.24–4.3 air_out')\n",
    "sns.kdeplot(data=test.air_out, shade=True, label='4.3–4.13 air_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(x=train.hum_in)\n",
    "plt.title('train hum_in')\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(x=train.hum_out)\n",
    "plt.title('train hum_out')\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(x=test.hum_in)\n",
    "plt.title('test hum_in')\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=test.hum_out)\n",
    "plt.title('test hum_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.lineplot(data=train.loc[train.time < '2019-3-24 01'].hum_in)\n",
    "plt.title('3.14–3.24 hum_in')\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.lineplot(data=train.loc[train.time < '2019-3-24 01'].hum_out)\n",
    "plt.title('3.14–3.24 hum_out')\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.lineplot(data=train.loc[train.time >= '2019-3-24 01'].hum_in)\n",
    "plt.title('3.24-4.3 hum_in')\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.lineplot(data=train.loc[train.time >= '2019-3-24 01'].hum_out)\n",
    "plt.title('3.24–4.3 hum_out')\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.lineplot(data=test.hum_in)\n",
    "plt.title('4.3-4.13 hum_in')\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.lineplot(data=test.hum_out)\n",
    "plt.title('4.3–4.13 hum_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.kdeplot(data=train.loc[train.time < '2019-3-24 01'].hum_in, shade=True, label='3.14–3.24 hum_in')\n",
    "sns.kdeplot(data=train.loc[train.time >= '2019-3-24 01'].hum_in, shade=True, label='3.24–4.3 hum_in')\n",
    "sns.kdeplot(data=test.hum_in, shade=True, label='4.3-4.13 hum_in')\n",
    "plt.subplot(1,2,2)\n",
    "sns.kdeplot(data=train.loc[train.time < '2019-3-24 01'].hum_out, shade=True, label='3.14–3.24 hum_out')\n",
    "sns.kdeplot(data=train.loc[train.time >= '2019-3-24 01'].hum_out, shade=True, label='3.24–4.3 hum_out')\n",
    "sns.kdeplot(data=test.hum_out, shade=True, label='4.3–4.13 hum_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.lineplot(data=train.loc[train.time < '2019-3-24 01'].temp_in)\n",
    "plt.title('3.14–3.24 temp_in')\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.lineplot(data=train.loc[train.time < '2019-3-24 01'].temp_out)\n",
    "plt.title('3.14–3.24 temp_out')\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.lineplot(data=train.loc[train.time >= '2019-3-24 01'].temp_in)\n",
    "plt.title('3.24-4.3 temp_in')\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.lineplot(data=train.loc[train.time >= '2019-3-24 01'].temp_out)\n",
    "plt.title('3.24–4.3 temp_out')\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.lineplot(data=test.temp_out)\n",
    "plt.title('4.3–4.13 temp_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.kdeplot(data=train.loc[train.time < '2019-3-24 01'].temp_in, shade=True, label='3.14–3.24 temp_in')\n",
    "sns.kdeplot(data=train.loc[train.time >= '2019-3-24 01'].temp_in, shade=True, label='3.24–4.3 temp_in')\n",
    "plt.subplot(1,2,2)\n",
    "sns.kdeplot(data=train.loc[train.time < '2019-3-24 01'].temp_out, shade=True, label='3.14–3.24 temp_out')\n",
    "sns.kdeplot(data=train.loc[train.time >= '2019-3-24 01'].temp_out, shade=True, label='3.24–4.3 temp_out')\n",
    "sns.kdeplot(data=test.temp_out, shade=True, label='4.3–4.13 temp_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 13.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# 基本聚合特征\n",
    "features = ['temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']\n",
    "group_feats = []\n",
    "for f in tqdm(features):\n",
    "    matrix['MDH_{}_medi'.format(f)] = matrix.groupby(['month','day','hour'])[f].transform('median')\n",
    "    matrix['MDH_{}_mean'.format(f)] = matrix.groupby(['month','day','hour'])[f].transform('mean')\n",
    "    matrix['MDH_{}_max'.format(f)] = matrix.groupby(['month','day','hour'])[f].transform('max')\n",
    "    matrix['MDH_{}_min'.format(f)] = matrix.groupby(['month','day','hour'])[f].transform('min')\n",
    "    matrix['MDH_{}_std'.format(f)] = matrix.groupby(['month','day','hour'])[f].transform('std')\n",
    "#     matrix['DH_{}_medi'.format(f)] = matrix.groupby(['day','hour'])[f].transform('median')\n",
    "#     matrix['DH_{}_mean'.format(f)] = matrix.groupby(['day','hour'])[f].transform('mean')\n",
    "#     matrix['DH_{}_max'.format(f)] = matrix.groupby(['day','hour'])[f].transform('max')\n",
    "#     matrix['DH_{}_min'.format(f)] = matrix.groupby(['day','hour'])[f].transform('min')\n",
    "#     matrix['DH_{}_std'.format(f)] = matrix.groupby(['day','hour'])[f].transform('std')\n",
    "\n",
    "    group_feats.append('MDH_{}_medi'.format(f))\n",
    "    group_feats.append('MDH_{}_mean'.format(f))\n",
    "#     group_feats.append('DH_{}_medi'.format(f))\n",
    "#     group_feats.append('DH_{}_mean'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 50.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# 基本交叉特征\n",
    "for f1 in tqdm(features + group_feats):\n",
    "    for f2 in features + group_feats:\n",
    "        if f1 != f2:\n",
    "            colname = '{}_{}_ratio'.format(f1, f2)\n",
    "            matrix[colname] = matrix[f1].values / matrix[f2].values\n",
    "\n",
    "matrix = matrix.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 28.47it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 26.64it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 28.09it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 27.35it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 26.10it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 28.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# 历史信息提取\n",
    "matrix['dt'] = matrix['day'].values + (matrix['month'].values - 3) * 31\n",
    "\n",
    "features = features + ['temp_in']\n",
    "for f in features:\n",
    "    tmp_df = pd.DataFrame()\n",
    "    for t in tqdm(range(15, 45)):\n",
    "        tmp = matrix[matrix['dt'] < t].groupby(['hour'])[f].agg({'mean'}).reset_index()\n",
    "        tmp.columns = ['hour', 'hit_{}_mean'.format(f)]\n",
    "        tmp['dt'] = t\n",
    "        tmp_df = tmp_df.append(tmp)\n",
    "    \n",
    "    matrix = matrix.merge(tmp_df, on=['dt', 'hour'], how='left')\n",
    "    \n",
    "matrix = matrix.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag features\n",
    "features = ['temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']\n",
    "lags = [1, 24]\n",
    "\n",
    "for l in tqdm(lags):\n",
    "    tmp = matrix[features+['time']].copy()\n",
    "    tmp.time += pd.Timedelta(str(l) + ' hour')\n",
    "    col_names = ['lag_'+str(l)+'_'+f for f in features]\n",
    "    tmp.columns = col_names + ['time']\n",
    "    matrix = pd.merge(matrix, tmp, on='time', how='left')\n",
    "    del tmp\n",
    "\n",
    "matrix.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend\n",
    "for feat in features:\n",
    "    for n in lags:\n",
    "        matrix['trend_'+str(n)+'_'+feat] = matrix[feat] - matrix['lag_'+str(n)+'_'+feat]\n",
    "\n",
    "matrix.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.78s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\n",
      "100%|██████████| 5/5 [00:14<00:00,  2.89s/it]\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "features = ['temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']\n",
    "for f in features:\n",
    "    matrix[f+'_20_bin'] = pd.cut(matrix[f], 20, duplicates='drop').apply(lambda x:x.left).astype(int)\n",
    "    matrix[f+'_50_bin'] = pd.cut(matrix[f], 50, duplicates='drop').apply(lambda x:x.left).astype(int)\n",
    "    matrix[f+'_100_bin'] = pd.cut(matrix[f], 100, duplicates='drop').apply(lambda x:x.left).astype(int)\n",
    "    matrix[f+'_200_bin'] = pd.cut(matrix[f], 200, duplicates='drop').apply(lambda x:x.left).astype(int)\n",
    "    \n",
    "features_20_bin = [f + '_20_bin' for f in features]\n",
    "for f1 in tqdm(features_20_bin):\n",
    "    for f2 in features:\n",
    "        matrix['{}_{}_medi'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('median')\n",
    "        matrix['{}_{}_mean'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('mean')\n",
    "        matrix['{}_{}_max'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('max')\n",
    "        matrix['{}_{}_min'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('min')\n",
    "\n",
    "features_50_bin = [f + '_50_bin' for f in features]\n",
    "for f1 in tqdm(features_50_bin):\n",
    "    for f2 in features:\n",
    "        matrix['{}_{}_medi'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('median')\n",
    "        matrix['{}_{}_mean'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('mean')\n",
    "        matrix['{}_{}_max'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('max')\n",
    "        matrix['{}_{}_min'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('min')\n",
    "\n",
    "features_100_bin = [f + '_100_bin' for f in features]\n",
    "for f1 in tqdm(features_100_bin):\n",
    "    for f2 in features:\n",
    "        matrix['{}_{}_medi'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('median')\n",
    "        matrix['{}_{}_mean'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('mean')\n",
    "        matrix['{}_{}_max'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('max')\n",
    "        matrix['{}_{}_min'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('min')\n",
    "\n",
    "features_200_bin = [f + '_200_bin' for f in features]\n",
    "for f1 in tqdm(features_200_bin):\n",
    "    for f2 in features:\n",
    "        matrix['{}_{}_medi'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('median')\n",
    "        matrix['{}_{}_mean'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('mean')\n",
    "        matrix['{}_{}_max'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('max')\n",
    "        matrix['{}_{}_min'.format(f1,f2)] = matrix.groupby([f1])[f2].transform('min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "features = ['temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']\n",
    "lags = [1,3,6,12,24]\n",
    "matrix.set_index('time', inplace=True)\n",
    "\n",
    "for feat in tqdm(features):\n",
    "    for l in lags:\n",
    "        lag_hour = str(l) + 'h'\n",
    "        matrix['mean_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).mean()\n",
    "        matrix['median_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).median()\n",
    "        matrix['max_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).max()\n",
    "        matrix['min_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).min()\n",
    "        matrix['std_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).std()\n",
    "        matrix['skew_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).skew()\n",
    "        matrix['q1_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).quantile(quantile=0.25)\n",
    "        matrix['q3_'+str(l)+'_hours_'+feat] = matrix[feat].rolling(lag_hour).median(quantile=0.75)\n",
    "        matrix['var_'+str(l)+'_hours_'+feat] = matrix['std_'+str(l)+'_hours_'+feat] / matrix['mean_'+str(l)+'_hours_'+feat]\n",
    "        \n",
    "matrix.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict trend with linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# data = pd.read_pickle('data.pkl')\n",
    "# features = ['month', 'day', 'hour', 'minute', 'temp_out', 'hum_out', 'air_out', 'hum_in', 'air_in']\n",
    "# num = len(train)\n",
    "# X_train = data[:num][features]\n",
    "# y_train = data[:num]['temp_in']\n",
    "# X_test = data[num:][features]\n",
    "# reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_train_lr = reg.predict(X_train)\n",
    "# pred_test_lr = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['timestamp', 'year', 'second', 'time', 'target', 'temp_in']\n",
    "\n",
    "num = int(len(train)*0.8)\n",
    "X_train = data.iloc[:num].drop(features_to_drop, axis=1)\n",
    "y_train = data.iloc[:num]['target']\n",
    "# y_train = data.iloc[:num]['temp_in'] - pred_train_lr[:num]\n",
    "X_val = data.iloc[num:len(train)].drop(features_to_drop, axis=1)\n",
    "y_val = data.iloc[num:len(train)]['target']\n",
    "# y_val = data.iloc[num:len(train)]['temp_in'] - pred_train_lr[num:len(train)]\n",
    "X_test = data.loc[data.time >= '2019-4-3 01'].drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # grid search\n",
    "\n",
    "# params = {\n",
    "#     'eta': [0.05, 0.1, 0.2],\n",
    "#     'max_depth': [7,8,9],\n",
    "#     'colsample_bytree': [0.6,0.7],\n",
    "#     'subsample': [0.6,0.7],\n",
    "#     'min_child_weight': [4,5,6]\n",
    "# }\n",
    "\n",
    "# best_score, best_param = 100, None\n",
    "\n",
    "# for i, p in enumerate(ParameterGrid(params)):\n",
    "#     model = XGBRegressor(max_depth=9,\n",
    "#                          n_estimators=100,\n",
    "#                          min_child_weight=0.5, \n",
    "#                          colsample_bytree=0.6, \n",
    "#                          subsample=0.6, \n",
    "#                          eta=0.1,\n",
    "#                          seed=10)\n",
    "#     model.set_params(**p)\n",
    "#     model.fit(X_train, \n",
    "#               y_train, \n",
    "#               eval_metric='mae', \n",
    "#               eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "#               verbose=False, \n",
    "#               early_stopping_rounds=20)\n",
    "#     pre_val = model.predict(X_val)\n",
    "#     score = mean_absolute_error(y_val, pre_val)\n",
    "#     print('round {}: {:.4f}'.format(i+1, score))\n",
    "#     print('params: {}'.format(p))\n",
    "#     print('\\n')\n",
    "#     if score < best_score:\n",
    "#         best_score = score\n",
    "#         best_param = p\n",
    "\n",
    "# print(best_score)\n",
    "# print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:0.64519\tvalidation_1-mae:0.34407\n",
      "Multiple eval metrics have been passed: 'validation_1-mae' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-mae hasn't improved in 1000 rounds.\n",
      "[500]\tvalidation_0-mae:0.40224\tvalidation_1-mae:0.24909\n",
      "[1000]\tvalidation_0-mae:0.25516\tvalidation_1-mae:0.20090\n",
      "[1500]\tvalidation_0-mae:0.16727\tvalidation_1-mae:0.17865\n",
      "[2000]\tvalidation_0-mae:0.11589\tvalidation_1-mae:0.16849\n",
      "[2500]\tvalidation_0-mae:0.08647\tvalidation_1-mae:0.16374\n",
      "[3000]\tvalidation_0-mae:0.06976\tvalidation_1-mae:0.16140\n",
      "[3500]\tvalidation_0-mae:0.06009\tvalidation_1-mae:0.15989\n",
      "[4000]\tvalidation_0-mae:0.05439\tvalidation_1-mae:0.15884\n",
      "[4500]\tvalidation_0-mae:0.05073\tvalidation_1-mae:0.15809\n",
      "[5000]\tvalidation_0-mae:0.04827\tvalidation_1-mae:0.15748\n",
      "[5500]\tvalidation_0-mae:0.04655\tvalidation_1-mae:0.15714\n",
      "[6000]\tvalidation_0-mae:0.04523\tvalidation_1-mae:0.15689\n",
      "[6500]\tvalidation_0-mae:0.04421\tvalidation_1-mae:0.15673\n",
      "[7000]\tvalidation_0-mae:0.04335\tvalidation_1-mae:0.15660\n",
      "[7500]\tvalidation_0-mae:0.04258\tvalidation_1-mae:0.15650\n",
      "[8000]\tvalidation_0-mae:0.04187\tvalidation_1-mae:0.15639\n",
      "[8500]\tvalidation_0-mae:0.04120\tvalidation_1-mae:0.15633\n",
      "[9000]\tvalidation_0-mae:0.04056\tvalidation_1-mae:0.15625\n",
      "[9500]\tvalidation_0-mae:0.03993\tvalidation_1-mae:0.15620\n",
      "[10000]\tvalidation_0-mae:0.03932\tvalidation_1-mae:0.15613\n",
      "[10500]\tvalidation_0-mae:0.03875\tvalidation_1-mae:0.15608\n",
      "[11000]\tvalidation_0-mae:0.03818\tvalidation_1-mae:0.15601\n",
      "[11500]\tvalidation_0-mae:0.03765\tvalidation_1-mae:0.15596\n",
      "[12000]\tvalidation_0-mae:0.03713\tvalidation_1-mae:0.15590\n",
      "[12500]\tvalidation_0-mae:0.03664\tvalidation_1-mae:0.15584\n",
      "[13000]\tvalidation_0-mae:0.03615\tvalidation_1-mae:0.15578\n",
      "[13500]\tvalidation_0-mae:0.03567\tvalidation_1-mae:0.15571\n",
      "[14000]\tvalidation_0-mae:0.03520\tvalidation_1-mae:0.15565\n",
      "[14500]\tvalidation_0-mae:0.03474\tvalidation_1-mae:0.15562\n",
      "[15000]\tvalidation_0-mae:0.03431\tvalidation_1-mae:0.15559\n",
      "[15500]\tvalidation_0-mae:0.03389\tvalidation_1-mae:0.15556\n",
      "[16000]\tvalidation_0-mae:0.03347\tvalidation_1-mae:0.15554\n",
      "[16500]\tvalidation_0-mae:0.03306\tvalidation_1-mae:0.15547\n",
      "[17000]\tvalidation_0-mae:0.03265\tvalidation_1-mae:0.15546\n",
      "[17500]\tvalidation_0-mae:0.03226\tvalidation_1-mae:0.15545\n",
      "[18000]\tvalidation_0-mae:0.03188\tvalidation_1-mae:0.15540\n",
      "[18500]\tvalidation_0-mae:0.03150\tvalidation_1-mae:0.15538\n",
      "[19000]\tvalidation_0-mae:0.03114\tvalidation_1-mae:0.15536\n",
      "[19500]\tvalidation_0-mae:0.03077\tvalidation_1-mae:0.15534\n",
      "[20000]\tvalidation_0-mae:0.03039\tvalidation_1-mae:0.15528\n",
      "[20500]\tvalidation_0-mae:0.03004\tvalidation_1-mae:0.15527\n",
      "[21000]\tvalidation_0-mae:0.02968\tvalidation_1-mae:0.15526\n",
      "[21500]\tvalidation_0-mae:0.02933\tvalidation_1-mae:0.15524\n",
      "[22000]\tvalidation_0-mae:0.02898\tvalidation_1-mae:0.15521\n",
      "[22500]\tvalidation_0-mae:0.02864\tvalidation_1-mae:0.15519\n",
      "[23000]\tvalidation_0-mae:0.02831\tvalidation_1-mae:0.15515\n",
      "[23500]\tvalidation_0-mae:0.02798\tvalidation_1-mae:0.15514\n",
      "[24000]\tvalidation_0-mae:0.02766\tvalidation_1-mae:0.15514\n",
      "[24500]\tvalidation_0-mae:0.02735\tvalidation_1-mae:0.15514\n",
      "[25000]\tvalidation_0-mae:0.02703\tvalidation_1-mae:0.15509\n",
      "[25500]\tvalidation_0-mae:0.02670\tvalidation_1-mae:0.15509\n",
      "[26000]\tvalidation_0-mae:0.02639\tvalidation_1-mae:0.15508\n",
      "[26500]\tvalidation_0-mae:0.02609\tvalidation_1-mae:0.15505\n",
      "[27000]\tvalidation_0-mae:0.02578\tvalidation_1-mae:0.15502\n",
      "[27500]\tvalidation_0-mae:0.02548\tvalidation_1-mae:0.15500\n",
      "[28000]\tvalidation_0-mae:0.02519\tvalidation_1-mae:0.15499\n",
      "[28500]\tvalidation_0-mae:0.02491\tvalidation_1-mae:0.15498\n",
      "[29000]\tvalidation_0-mae:0.02462\tvalidation_1-mae:0.15496\n",
      "[29500]\tvalidation_0-mae:0.02434\tvalidation_1-mae:0.15494\n",
      "[30000]\tvalidation_0-mae:0.02407\tvalidation_1-mae:0.15493\n",
      "[30500]\tvalidation_0-mae:0.02380\tvalidation_1-mae:0.15493\n",
      "[31000]\tvalidation_0-mae:0.02354\tvalidation_1-mae:0.15493\n",
      "Stopping. Best iteration:\n",
      "[30044]\tvalidation_0-mae:0.02404\tvalidation_1-mae:0.15492\n",
      "\n",
      "CPU times: user 8h 26min 28s, sys: 1min 6s, total: 8h 27min 35s\n",
      "Wall time: 4h 15min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.5, eta=0.001, gamma=0,\n",
       "             gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.00100000005, max_delta_step=0, max_depth=8,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=50000, n_jobs=2, num_parallel_tree=1,\n",
       "             random_state=2020, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=2020, subsample=0.5, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = XGBRegressor(max_depth=8,\n",
    "                     n_estimators=50000,\n",
    "                     min_child_weight=5, \n",
    "                     colsample_bytree=0.5, \n",
    "                     subsample=0.5, \n",
    "                     eta=0.001,\n",
    "                     seed=2020)\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          eval_metric='mae', \n",
    "          eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "          verbose=500, \n",
    "          early_stopping_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_xgb = model.predict(X_test, ntree_limit=model.best_ntree_limit)\n",
    "submission = pd.DataFrame({'time': test.timestamp, \n",
    "                           'temperature': test.temp_out + pred_test_xgb})\n",
    "submission.to_csv('submissions/xgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['timestamp', 'year', 'second', 'time', 'target', 'temp_in']\n",
    "\n",
    "X_train = data.loc[data.time < '2019-4-3 01'].drop(features_to_drop, axis=1)\n",
    "y_train = data.loc[data.time < '2019-4-3 01']['target']\n",
    "X_test = data.loc[data.time >= '2019-4-3 01'].drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:0.58496\tvalidation_1-mae:0.58496\n",
      "Multiple eval metrics have been passed: 'validation_1-mae' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-mae hasn't improved in 1000 rounds.\n",
      "[500]\tvalidation_0-mae:0.36758\tvalidation_1-mae:0.36758\n",
      "[1000]\tvalidation_0-mae:0.23616\tvalidation_1-mae:0.23616\n",
      "[1500]\tvalidation_0-mae:0.15802\tvalidation_1-mae:0.15802\n",
      "[2000]\tvalidation_0-mae:0.11232\tvalidation_1-mae:0.11232\n",
      "[2500]\tvalidation_0-mae:0.08608\tvalidation_1-mae:0.08608\n",
      "[2999]\tvalidation_0-mae:0.07106\tvalidation_1-mae:0.07106\n",
      "CPU times: user 1h 5min 25s, sys: 13.2 s, total: 1h 5min 38s\n",
      "Wall time: 33min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.5, eta=0.001, gamma=0,\n",
       "             gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.00100000005, max_delta_step=0, max_depth=8,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=3000, n_jobs=0, num_parallel_tree=1,\n",
       "             random_state=2020, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=2020, subsample=0.5, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = XGBRegressor(max_depth=8,\n",
    "                     n_estimators=3000,\n",
    "                     min_child_weight=5, \n",
    "                     colsample_bytree=0.5, \n",
    "                     subsample=0.5, \n",
    "                     eta=0.001,\n",
    "                     seed=2020)\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          eval_metric='mae', \n",
    "          eval_set=[(X_train, y_train), (X_train, y_train)], \n",
    "          verbose=500, \n",
    "          early_stopping_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_xgb = model.predict(X_test, ntree_limit=model.best_ntree_limit)\n",
    "submission = pd.DataFrame({'time': test.timestamp, \n",
    "                           'temperature': test.temp_out + pred_test_xgb})\n",
    "submission.to_csv('submissions/xgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
